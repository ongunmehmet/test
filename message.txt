[1]	GPT-4 technical report, OpenAI, 2023. Accessed: Sep. 19, 2024. [Online.] Available: https://cdn.openai.com/papers/gpt-4.pdf
[2]	GPT-4o mini: advancing cost-efficient intelligence, OpenAI. Accessed: Sep. 19, 2024. [Online]. Available: https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/
[3]	Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou, Z., Gong, C., Shen, Y. and Zhou, J., 2023. A comprehensive capability analysis of gpt-3 and gpt-3.5 series models. arXiv preprint arXiv:2303.10420.
[4]	Westermann, H., Dallma: Semi-Structured Legal Reasoning and Drafting with Large Language Models.
[5]	Shin, J., Tang, C., Mohati, T., Nayebi, M., Wang, S. and Hemmati, H., 2023. Prompt engineering or fine tuning: An empirical assessment of large language models in automated software engineering tasks. arXiv preprint arXiv:2310.10508.
[6]	Pornprasit, C. and Tantithamthavorn, C., 2024. Fine-tuning and prompt engineering for large language models-based code review automation. Information and Software Technology, 175, p.107523. 
[7]	SentenceTransformers Documentation, Accessed: Sep. 19, 2024. [Online.] Available: https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html
[8]	Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y. and Ye, W., 2024. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 15(3), pp.1-45. 
[9]	T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., 2020. Accessed: Sep. 19, 2024. [Online.] Available: https://splab.sdu.edu.cn/GPT3.pdf 
[10]	Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F. and Rodriguez, A., 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.
[11]	Hello, Gpt-4o, OpenAI. Accessed: Sep. 5, 2024. [Online.] Available: https://openai.com/index/hello-gpt-4o/ 
[12]	Kojima, T., Gu, S.S., Reid, M., Matsuo, Y. and Iwasawa, Y., 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35, pp.22199-22213. 
[13]	Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V. and Zhou, D., 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35, pp. 24824-24837. 
[14]	Zhang, B., Liu, Z., Cherry, C. and Firat, O., 2024. When scaling meets llm finetuning: The effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193.
[15]	Moundas, M., White, J. and Schmidt, D.C., Prompt Patterns for Structured Data Extraction from Unstructured Text. 
[16]	Musumeci, E., Brienza, M., Suriani, V., Nardi, D. and Bloisi, D.D.,  LLM Based Multi-agent Generation of Semi-structured Documents from Semantic Templates in the Public Administration Domain. In International Conference on Human-Computer Interaction, May. 2024, pp. 98-117. Cham: Springer Nature Switzerland.
[17]	Fine-tuning: Preparing Your Dataset, OpenAI. Accessed: Sep. 17, 2024. [Online.] Available: https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset 
[18]	Topsakal, O. and Akinci, T.C., Creating large language model applications utilizing langchain: A primer on developing llm apps fast. In International Conference on Applied Engineering and Natural Sciences vol. 1, no. 1, July. 2023, pp. 1050-1056.
[19]	Sentence Transformers Library, Pretrained Models. Accessed: Sep. 16, 2024. [Online.] Available:  https://www.sbert.net/docs/sentence_transformer/pretrained_models.html
[20]	Introducing Claude. Accessed: Sep. 16, 2024. [Online.] Available:  https://www.anthropic.com/news/introducing-claude 
Fine-tuning: Use a fine-tuned model, OpenAI. Accessed: Sep. 16, 2024. Available: https://platform.openai.com/docs/guides/fine-tuning/use-a-fine-tuned-model
